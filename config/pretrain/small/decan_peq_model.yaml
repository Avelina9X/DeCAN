model:
  vocab_size: 50272
  vocab_dim: 768
  hidden_size: 768
  intermediate_size: 2752
  intermediate_act: silu
  num_hidden_layers: 12
  num_key_value_heads: 1
  num_attention_heads: 12
  head_dim: 64
  max_position_embeddings: 4096
  rope_theta: 500000
  rope_scaling: {}
  tie_word_embeddings: True
  attention_bias: True
  mlp_bias: True
  initializer_range: 0.02
  rms_norm_eps: 1.0e-6
  use_cache: True
  bos_token_id: 2
  eos_token_id: 2
  pad_token_id: 1
  sep_token_id: null
  cls_token_id: null
